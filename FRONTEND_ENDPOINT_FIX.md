# Frontend Endpoint Configuration - Fixed ‚úÖ

## Configuration Status

### ‚úÖ Fixed Files
1. **`frontend/next.config.ts`**
   - `NEXT_PUBLIC_API_URL`: `http://localhost:3001` ‚úì

2. **`frontend/app/chat/page.tsx`**
   - `API_URL`: Uses `NEXT_PUBLIC_API_URL` or defaults to `http://localhost:3001` ‚úì
   - Calls: `POST ${API_URL}/api/chat/stream` ‚úì

## Connection Architecture

```
Frontend (Next.js :3000)
    ‚Üì
    Calls: POST /api/chat/stream
    ‚Üì
Backend API (Express :3001)  ‚Üê Frontend connects here ‚úÖ
    ‚Üì
    Calls: POST /api/generate
    ‚Üì
Ollama API Server (Python Flask :8080)
    ‚Üì
    Calls: POST /api/generate
    ‚Üì
Ollama (Docker :11434)
```

## Endpoint Mapping

### Frontend ‚Üí Backend
- **Frontend calls**: `POST http://localhost:3001/api/chat/stream`
- **Backend provides**: `/api/chat/stream` endpoint ‚úì
- **Request format**:
  ```json
  {
    "query": "user question",
    "chatHistory": [],
    "topK": 5
  }
  ```
- **Response format**: Server-Sent Events (SSE)
  ```
  data: {"type":"sources","sources":[...]}
  data: {"type":"token","content":"..."}
  data: {"type":"suggestions","suggestions":[...]}
  data: {"type":"done"}
  ```

### Backend ‚Üí Ollama API Server
- **Backend calls**: `POST http://localhost:8080/api/generate`
- **Ollama API Server provides**: `/api/generate` endpoint ‚úì
- **Request format**:
  ```json
  {
    "prompt": "formatted prompt",
    "model": "gemma3:12b"
  }
  ```

## Verification

All configurations are now consistent:
- ‚úÖ Frontend points to backend (3001)
- ‚úÖ Frontend calls correct endpoint (`/api/chat/stream`)
- ‚úÖ Backend endpoint exists and is properly configured
- ‚úÖ Backend connects to Ollama API Server (8080)
- ‚úÖ Ollama API Server connects to Ollama (11434)

## Next Steps

1. **Start Backend** (if not running):
   ```bash
   cd backend
   npm run dev
   ```

2. **Start Frontend**:
   ```bash
   cd frontend
   npm run dev
   ```

3. **Open Browser**:
   - Navigate to: `http://localhost:3000/chat`
   - The frontend will connect to backend at `http://localhost:3001`
   - Backend will handle RAG and connect to Ollama API Server

## Environment Variables

If you need to override the default:

```bash
# In frontend/.env.local (optional)
NEXT_PUBLIC_API_URL=http://localhost:3001
```

The configuration is now correct and ready to use! üéâ
